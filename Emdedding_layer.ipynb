{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CV2Y0u_AV4uQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=10,output_dim=4,input_length=2)\n",
        "model.add(embedding_layer)\n",
        "model.compile('adam','mse')"
      ],
      "metadata": {
        "id": "OoqB3fN0V909"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = np.array([[1,2]])\n",
        "pred = model.predict(input_data)\n",
        "print(input_data.shape)\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvJz647mWD6A",
        "outputId": "6cfd7d1d-f2a1-45c2-f700-b7088eb58e69"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 93ms/step\n",
            "(1, 2)\n",
            "[[[ 0.044051    0.04555774  0.03470489 -0.03525679]\n",
            "  [ 0.02662767  0.00593636 -0.00188937  0.0365881 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# first [ for array second one for aggregate two vectors\n",
        "# [[\n",
        "  # The vector For word 1 : [ 0.044051    0.04555774  0.03470489 -0.03525679]\n",
        " # The vector For word 2 : [ 0.02662767  0.00593636 -0.00188937  0.0365881 ]\n",
        " # first [ for array second one for aggregate two vectors ]]\n",
        " # As we see the length of output is 4."
      ],
      "metadata": {
        "id": "d9MZrERYWHvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# or access the embedding layer through the constructed model\n",
        "# first `0` refers to the position of embedding layer in the `model`\n",
        "# embeddings = model.layers[0].get_weights()[0]\n",
        "embeddings = model.layers[0].get_weights()\n",
        "embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2FhlQNFWnSV",
        "outputId": "322ebed4-d2df-4412-f8c0-b8e4d20daa1f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.01971984, -0.04984335,  0.02057571, -0.03237033],\n",
              "        [ 0.044051  ,  0.04555774,  0.03470489, -0.03525679],\n",
              "        [ 0.02662767,  0.00593636, -0.00188937,  0.0365881 ],\n",
              "        [ 0.00802473, -0.00595004, -0.02466425,  0.01850195],\n",
              "        [ 0.04209392, -0.00069617, -0.04870185,  0.02699608],\n",
              "        [-0.03689731, -0.03607271, -0.04325372, -0.00563365],\n",
              "        [ 0.03870176,  0.04522774,  0.03089739, -0.01539282],\n",
              "        [-0.00060452, -0.00548015,  0.04173461,  0.01630619],\n",
              "        [ 0.04565387,  0.01521132, -0.02844551, -0.01636089],\n",
              "        [-0.02075909,  0.03396681, -0.01516876, -0.02283299]],\n",
              "       dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The output above is refer two 10 wocabulary size that we set and each of row is represent the vector of words. that is important that the weights are above are initialize randomly and every time you run the cell get different numbers.\n",
        "# for example :\n",
        "# The first word (0) is represented by first row in this table, which is :\n",
        "# [-0.04333381, -0.02326865, -0.00812379,  0.02167496]"
      ],
      "metadata": {
        "id": "ZCqA0enKX6p4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An example above has a problem because we don't train the embedding layer but in real text dataset we shold have done this. therefor in other eample we should see how train embedding layer."
      ],
      "metadata": {
        "id": "1YM6vg5jY8hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   Tokenize the sentences into words.\n",
        "2.   Create one-hot encoded vector for each word.\n",
        "1.   Use padding to ensure all sequences are of same length.\n",
        "2.   Pass the padded sequences as input to embedding layer.\n",
        "1.   Flatten and apply Dense layer to predict the label.:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B9F6QSJXaM0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten,Embedding,Dense"
      ],
      "metadata": {
        "id": "bc5nJKVfY7wh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define 10 restaurant reviews\n",
        "reviews =[\n",
        "          'Never coming back!',\n",
        "          'horrible service',\n",
        "          'rude waitress',\n",
        "          'cold food',\n",
        "          'horrible food!',\n",
        "          'awesome',\n",
        "          'awesome services!',\n",
        "          'rocks',\n",
        "          'poor work',\n",
        "          'couldn\\'t have done better'\n",
        "]\n",
        "#Define labels\n",
        "labels = array([1,1,1,1,1,0,0,0,0,0])"
      ],
      "metadata": {
        "id": "Ge2US1cYXF-g"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ILyqDNhjbuxC",
        "outputId": "7d7924a1-5ffe-4099-940d-5c106c1ff224"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Never coming back!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Vocab_size = 50\n",
        "encoded_reviews = [one_hot(d,Vocab_size) for d in reviews]\n",
        "print(f'encoded reviews: {encoded_reviews}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MhRuM4pbFdy",
        "outputId": "2a195099-0117-4ce7-de97-beb14f062c66"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded reviews: [[15, 25, 11], [44, 15], [7, 36], [10, 33], [44, 33], [48], [48, 48], [4], [15, 21], [42, 12, 7, 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot(reviews[0],Vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mhF3fI-bagh",
        "outputId": "454973a1-33dd-42c3-9587-eda452f38050"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[15, 25, 11]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Difference between post and pre is in post use 0 at the end but in pre use 0 at the begining."
      ],
      "metadata": {
        "id": "IZcnXb7T8hi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "padded_reviews = pad_sequences(encoded_reviews,maxlen=max_length,padding='post')\n",
        "print(padded_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZjz2s-FbzxX",
        "outputId": "114146c2-32eb-493c-82b9-cbfc4461e52a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[15 25 11  0]\n",
            " [44 15  0  0]\n",
            " [ 7 36  0  0]\n",
            " [10 33  0  0]\n",
            " [44 33  0  0]\n",
            " [48  0  0  0]\n",
            " [48 48  0  0]\n",
            " [ 4  0  0  0]\n",
            " [15 21  0  0]\n",
            " [42 12  7 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "padded_reviews = pad_sequences(encoded_reviews,maxlen=max_length,padding='pre')\n",
        "print(padded_reviews)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtgxX5UD8bzx",
        "outputId": "b983ea96-c008-4d35-e195-a00dac33611d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0 15 25 11]\n",
            " [ 0  0 44 15]\n",
            " [ 0  0  7 36]\n",
            " [ 0  0 10 33]\n",
            " [ 0  0 44 33]\n",
            " [ 0  0  0 48]\n",
            " [ 0  0 48 48]\n",
            " [ 0  0  0  4]\n",
            " [ 0  0 15 21]\n",
            " [42 12  7 40]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# After padding it is time to pass input to the embedding layer.\n",
        "# We will fix the length of embedded vectors for each word as 8 and the input length will be the maximum length which we have already defined as 4."
      ],
      "metadata": {
        "id": "FeHYs6eA9h0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "embedding_layer = Embedding(input_dim=Vocab_size,output_dim=8,input_length=max_length)\n",
        "model.add(embedding_layer)\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['acc'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGMddZW-9g3H",
        "outputId": "fff3f322-c7a9-4f6b-bbdd-ec8a19730d9b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 4, 8)              400       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 433 (1.69 KB)\n",
            "Trainable params: 433 (1.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "Hx91O71X_E3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_reviews,labels,epochs=100,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHIDbkbZ_Iiw",
        "outputId": "6c10f28d-970e-4790-e9db-5ced9799adaa"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4529 - acc: 1.0000\n",
            "Epoch 2/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.4501 - acc: 1.0000\n",
            "Epoch 3/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4473 - acc: 1.0000\n",
            "Epoch 4/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4445 - acc: 1.0000\n",
            "Epoch 5/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4417 - acc: 1.0000\n",
            "Epoch 6/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4389 - acc: 1.0000\n",
            "Epoch 7/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4361 - acc: 1.0000\n",
            "Epoch 8/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4333 - acc: 1.0000\n",
            "Epoch 9/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4305 - acc: 1.0000\n",
            "Epoch 10/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4277 - acc: 1.0000\n",
            "Epoch 11/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4249 - acc: 1.0000\n",
            "Epoch 12/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4221 - acc: 1.0000\n",
            "Epoch 13/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4194 - acc: 1.0000\n",
            "Epoch 14/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4166 - acc: 1.0000\n",
            "Epoch 15/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4138 - acc: 1.0000\n",
            "Epoch 16/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4110 - acc: 1.0000\n",
            "Epoch 17/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4083 - acc: 1.0000\n",
            "Epoch 18/100\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4055 - acc: 1.0000\n",
            "Epoch 19/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4028 - acc: 1.0000\n",
            "Epoch 20/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4000 - acc: 1.0000\n",
            "Epoch 21/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3973 - acc: 1.0000\n",
            "Epoch 22/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3945 - acc: 1.0000\n",
            "Epoch 23/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3918 - acc: 1.0000\n",
            "Epoch 24/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3891 - acc: 1.0000\n",
            "Epoch 25/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3863 - acc: 1.0000\n",
            "Epoch 26/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3836 - acc: 1.0000\n",
            "Epoch 27/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3809 - acc: 1.0000\n",
            "Epoch 28/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3782 - acc: 1.0000\n",
            "Epoch 29/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3755 - acc: 1.0000\n",
            "Epoch 30/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3729 - acc: 1.0000\n",
            "Epoch 31/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3702 - acc: 1.0000\n",
            "Epoch 32/100\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3675 - acc: 1.0000\n",
            "Epoch 33/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3649 - acc: 1.0000\n",
            "Epoch 34/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3622 - acc: 1.0000\n",
            "Epoch 35/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3596 - acc: 1.0000\n",
            "Epoch 36/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3570 - acc: 1.0000\n",
            "Epoch 37/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3543 - acc: 1.0000\n",
            "Epoch 38/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3517 - acc: 1.0000\n",
            "Epoch 39/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3491 - acc: 1.0000\n",
            "Epoch 40/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3465 - acc: 1.0000\n",
            "Epoch 41/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3440 - acc: 1.0000\n",
            "Epoch 42/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3414 - acc: 1.0000\n",
            "Epoch 43/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3388 - acc: 1.0000\n",
            "Epoch 44/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3363 - acc: 1.0000\n",
            "Epoch 45/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3338 - acc: 1.0000\n",
            "Epoch 46/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3313 - acc: 1.0000\n",
            "Epoch 47/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3287 - acc: 1.0000\n",
            "Epoch 48/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3262 - acc: 1.0000\n",
            "Epoch 49/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3238 - acc: 1.0000\n",
            "Epoch 50/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3213 - acc: 1.0000\n",
            "Epoch 51/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3188 - acc: 1.0000\n",
            "Epoch 52/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3164 - acc: 1.0000\n",
            "Epoch 53/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3139 - acc: 1.0000\n",
            "Epoch 54/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.3115 - acc: 1.0000\n",
            "Epoch 55/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3091 - acc: 1.0000\n",
            "Epoch 56/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3067 - acc: 1.0000\n",
            "Epoch 57/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3043 - acc: 1.0000\n",
            "Epoch 58/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3020 - acc: 1.0000\n",
            "Epoch 59/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2996 - acc: 1.0000\n",
            "Epoch 60/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2973 - acc: 1.0000\n",
            "Epoch 61/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2949 - acc: 1.0000\n",
            "Epoch 62/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2926 - acc: 1.0000\n",
            "Epoch 63/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2903 - acc: 1.0000\n",
            "Epoch 64/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2880 - acc: 1.0000\n",
            "Epoch 65/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2858 - acc: 1.0000\n",
            "Epoch 66/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2835 - acc: 1.0000\n",
            "Epoch 67/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2813 - acc: 1.0000\n",
            "Epoch 68/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2790 - acc: 1.0000\n",
            "Epoch 69/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2768 - acc: 1.0000\n",
            "Epoch 70/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2746 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2724 - acc: 1.0000\n",
            "Epoch 72/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2702 - acc: 1.0000\n",
            "Epoch 73/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2681 - acc: 1.0000\n",
            "Epoch 74/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2659 - acc: 1.0000\n",
            "Epoch 75/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2638 - acc: 1.0000\n",
            "Epoch 76/100\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2617 - acc: 1.0000\n",
            "Epoch 77/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2596 - acc: 1.0000\n",
            "Epoch 78/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2575 - acc: 1.0000\n",
            "Epoch 79/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2554 - acc: 1.0000\n",
            "Epoch 80/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2534 - acc: 1.0000\n",
            "Epoch 81/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2513 - acc: 1.0000\n",
            "Epoch 82/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2493 - acc: 1.0000\n",
            "Epoch 83/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2473 - acc: 1.0000\n",
            "Epoch 84/100\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2453 - acc: 1.0000\n",
            "Epoch 85/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2433 - acc: 1.0000\n",
            "Epoch 86/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2413 - acc: 1.0000\n",
            "Epoch 87/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2394 - acc: 1.0000\n",
            "Epoch 88/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2374 - acc: 1.0000\n",
            "Epoch 89/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2355 - acc: 1.0000\n",
            "Epoch 90/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2336 - acc: 1.0000\n",
            "Epoch 91/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2317 - acc: 1.0000\n",
            "Epoch 92/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2298 - acc: 1.0000\n",
            "Epoch 93/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2280 - acc: 1.0000\n",
            "Epoch 94/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2261 - acc: 1.0000\n",
            "Epoch 95/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2243 - acc: 1.0000\n",
            "Epoch 96/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2225 - acc: 1.0000\n",
            "Epoch 97/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2207 - acc: 1.0000\n",
            "Epoch 98/100\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2189 - acc: 1.0000\n",
            "Epoch 99/100\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2171 - acc: 1.0000\n",
            "Epoch 100/100\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2153 - acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e9acd64dea0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lets check the shape of the weight matrix."
      ],
      "metadata": {
        "id": "Lce20t-C_QH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.get_weights()[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EPwaMkd_QdJ",
        "outputId": "b66656d0-6498-4233-9aa0-87f622cd70e1"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.get_weights())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jXNjyRH_sVk",
        "outputId": "99cedc99-573f-4600-c980-6336f7a12713"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[-2.31203958e-01,  1.86820880e-01,  1.68216407e-01,\n",
            "        -2.04175085e-01,  1.68387517e-01, -2.67770767e-01,\n",
            "        -1.87188357e-01, -2.19602570e-01],\n",
            "       [ 2.65760086e-02,  2.87699699e-03, -3.34760435e-02,\n",
            "         4.25954908e-03,  3.84280719e-02, -4.34306040e-02,\n",
            "        -1.71516165e-02,  4.92638014e-02],\n",
            "       [-2.32205037e-02,  4.60121073e-02,  3.79404165e-02,\n",
            "         2.62589380e-03, -4.00102623e-02,  7.34820217e-03,\n",
            "         2.63142847e-02,  3.72612812e-02],\n",
            "       [ 2.03385614e-02,  1.11434348e-02,  1.05656311e-03,\n",
            "        -7.76886940e-04, -2.26223823e-02,  1.85558088e-02,\n",
            "        -4.89340685e-02, -7.93641806e-03],\n",
            "       [ 1.78463817e-01, -1.58793807e-01, -1.91601187e-01,\n",
            "         1.45624533e-01,  1.92026883e-01, -1.69741556e-01,\n",
            "        -1.88673213e-01, -1.89551279e-01],\n",
            "       [-5.94462082e-03, -5.18176705e-03, -4.69034687e-02,\n",
            "        -1.27081163e-02,  3.43217291e-02,  1.17473714e-02,\n",
            "        -3.95879634e-02,  3.30544375e-02],\n",
            "       [-1.01365559e-02,  9.39179212e-04,  3.56924050e-02,\n",
            "        -2.72853617e-02, -2.10762974e-02,  1.94914602e-02,\n",
            "        -1.84198394e-02, -4.69745174e-02],\n",
            "       [ 2.49801636e-01, -2.73958653e-01, -2.11264074e-01,\n",
            "         2.63283968e-01, -2.05020577e-01,  2.68043607e-01,\n",
            "         2.62497365e-01,  2.36981094e-01],\n",
            "       [ 9.45105404e-03,  3.47189195e-02,  2.35862993e-02,\n",
            "         3.35093401e-02,  1.03468783e-02,  3.53355668e-02,\n",
            "        -1.30689144e-02,  1.97732486e-02],\n",
            "       [ 7.81389326e-03, -3.99638526e-02,  2.99256556e-02,\n",
            "        -2.93199066e-02,  4.41101231e-02, -4.75958250e-02,\n",
            "         1.05914250e-02, -1.17374659e-02],\n",
            "       [ 2.07816839e-01, -2.47884750e-01, -2.33530596e-01,\n",
            "         2.91548610e-01, -1.75474346e-01,  2.55868196e-01,\n",
            "         2.18722656e-01,  1.81952626e-01],\n",
            "       [-1.50626272e-01,  1.82271570e-01,  1.76701546e-01,\n",
            "        -1.99350446e-01, -1.42008603e-01,  1.72943741e-01,\n",
            "         2.14739308e-01,  1.40581161e-01],\n",
            "       [ 1.82182893e-01,  2.52128065e-01, -2.39894807e-01,\n",
            "         1.58587575e-01,  1.63466245e-01, -1.40519917e-01,\n",
            "        -2.18093529e-01, -2.10111350e-01],\n",
            "       [ 2.50427388e-02,  2.59747766e-02, -3.47306579e-03,\n",
            "         4.23986353e-02, -4.58292849e-02,  3.30610536e-02,\n",
            "         4.71094996e-03, -2.87013780e-02],\n",
            "       [-1.31239407e-02,  4.26016338e-02, -8.69866461e-03,\n",
            "         1.89977176e-02, -3.14358473e-02,  2.56937630e-02,\n",
            "        -3.97546999e-02,  3.98945808e-03],\n",
            "       [-1.82255566e-01,  2.21520409e-01,  2.07143113e-01,\n",
            "        -2.39930823e-01, -1.88313350e-01,  1.37641549e-01,\n",
            "         2.22712398e-01,  1.56510755e-01],\n",
            "       [-1.85062401e-02,  6.30126148e-03,  4.22218554e-02,\n",
            "         4.79194783e-02,  1.71453692e-02, -2.73124576e-02,\n",
            "        -1.86064951e-02,  4.09149416e-02],\n",
            "       [-2.07962990e-02,  1.03582069e-03, -2.89297942e-02,\n",
            "         4.75752465e-02,  1.72641613e-02,  1.25363730e-02,\n",
            "        -1.25064142e-02, -4.92950939e-02],\n",
            "       [-3.61982957e-02, -8.66745785e-03, -6.00670651e-03,\n",
            "         5.10047749e-03,  2.83903815e-02, -1.88132059e-02,\n",
            "         4.26291488e-02, -1.50129199e-02],\n",
            "       [ 1.01659298e-02,  1.03311613e-03, -1.28183961e-02,\n",
            "         4.99705337e-02, -4.97772098e-02, -7.52090290e-03,\n",
            "         1.67303123e-02,  3.68916728e-02],\n",
            "       [ 2.48863734e-02,  3.49241830e-02, -4.39942963e-02,\n",
            "         5.93329594e-03, -1.71467289e-02,  3.95361297e-02,\n",
            "         3.98961566e-02, -2.01462749e-02],\n",
            "       [ 2.23190457e-01, -1.90343797e-01, -2.24954575e-01,\n",
            "         1.74910843e-01,  2.38068044e-01, -1.76485837e-01,\n",
            "        -1.64398104e-01, -1.98970735e-01],\n",
            "       [ 1.34781338e-02, -7.64756277e-03, -2.80349609e-02,\n",
            "        -1.72348991e-02,  4.37610783e-02,  1.57080330e-02,\n",
            "         2.95647234e-03,  1.90670826e-02],\n",
            "       [ 2.33548880e-03,  2.36554630e-02, -1.53340399e-04,\n",
            "        -5.63394278e-04, -7.62090832e-03,  1.83747895e-02,\n",
            "         4.50436957e-02, -4.55865376e-02],\n",
            "       [ 4.53957170e-03, -5.73054701e-03,  1.67600401e-02,\n",
            "         3.71820070e-02, -9.36643034e-03, -4.26080711e-02,\n",
            "        -3.28021422e-02,  2.68753655e-02],\n",
            "       [ 2.27854967e-01, -1.84193209e-01, -1.46166474e-01,\n",
            "         2.69331366e-01, -1.89902037e-01,  2.14806065e-01,\n",
            "         2.31705233e-01,  1.52960837e-01],\n",
            "       [ 4.78259437e-02,  2.65535824e-02, -3.34130675e-02,\n",
            "        -3.31106074e-02,  3.30001228e-02,  1.21317804e-04,\n",
            "         2.40666382e-02, -4.13135290e-02],\n",
            "       [-4.94787693e-02,  4.29337136e-02,  4.18828465e-02,\n",
            "        -1.93979274e-02,  4.81207855e-02, -4.47768234e-02,\n",
            "        -4.30231951e-02, -1.75042376e-02],\n",
            "       [-3.72689366e-02, -3.49712372e-03, -1.67962909e-02,\n",
            "        -3.72283831e-02,  1.82308219e-02,  2.13790424e-02,\n",
            "         4.06298675e-02,  3.26243080e-02],\n",
            "       [ 2.39909813e-03,  3.75497229e-02,  3.14800031e-02,\n",
            "        -1.18994936e-02,  3.36364023e-02, -4.64232937e-02,\n",
            "         8.61383975e-04,  2.53069736e-02],\n",
            "       [-2.50720736e-02,  4.09111269e-02, -3.65869179e-02,\n",
            "         1.40589364e-02,  2.63244621e-02,  4.81451415e-02,\n",
            "         2.76852287e-02,  2.41177715e-02],\n",
            "       [-3.96754034e-02,  2.60536410e-02, -4.91433404e-02,\n",
            "        -2.14855801e-02,  2.00237744e-02,  3.36987115e-02,\n",
            "         7.72545487e-03, -3.50958109e-02],\n",
            "       [-1.59174800e-02,  2.71473080e-03, -4.63643931e-02,\n",
            "         2.68425792e-03, -3.18532065e-03,  3.18778642e-02,\n",
            "        -3.48624699e-02, -1.63118728e-02],\n",
            "       [-1.80509567e-01,  1.93292007e-01,  2.03652024e-01,\n",
            "        -1.98023021e-01, -2.03064501e-01,  2.11437717e-01,\n",
            "         2.13813126e-01,  2.21800268e-01],\n",
            "       [-1.44946948e-02, -6.07311726e-03,  4.60589640e-02,\n",
            "         4.15661819e-02,  2.15146057e-02,  3.39506753e-02,\n",
            "         1.33410357e-02, -3.08750402e-02],\n",
            "       [ 2.60361098e-02, -8.82310793e-03,  3.67704518e-02,\n",
            "        -2.18705181e-02, -1.55273303e-02,  4.25977744e-02,\n",
            "         3.73229720e-02,  3.46955322e-02],\n",
            "       [-2.58388281e-01,  2.36243129e-01,  1.52246848e-01,\n",
            "        -2.28499785e-01, -1.71481863e-01,  1.85830593e-01,\n",
            "         1.70495868e-01,  1.66933954e-01],\n",
            "       [-2.84087416e-02,  2.23318227e-02,  3.75896804e-02,\n",
            "        -3.68357822e-03, -1.04712732e-02,  3.94718535e-02,\n",
            "        -6.64259121e-03, -3.11625600e-02],\n",
            "       [ 8.74253362e-03,  4.43917029e-02, -4.30054553e-02,\n",
            "        -2.45621931e-02,  1.81657337e-02,  2.88932659e-02,\n",
            "        -3.91068086e-02,  1.59267671e-02],\n",
            "       [-1.45937130e-03,  1.36760511e-02, -4.41271923e-02,\n",
            "         1.68288238e-02,  3.41072343e-02, -1.05105340e-04,\n",
            "        -1.05252042e-02,  1.95430778e-02],\n",
            "       [ 1.62519291e-01, -2.04446256e-01, -2.37251043e-01,\n",
            "         2.30562329e-01,  2.06658393e-01, -2.09441885e-01,\n",
            "        -1.47590354e-01, -2.24725932e-01],\n",
            "       [ 4.18613069e-02, -3.22634466e-02,  2.59899981e-02,\n",
            "        -4.00506631e-02, -5.75784594e-03,  1.83833763e-03,\n",
            "         4.68334816e-02,  4.45571057e-02],\n",
            "       [ 2.07144022e-01,  2.08186254e-01, -1.93559691e-01,\n",
            "        -2.83918291e-01, -2.48295560e-01, -2.37025648e-01,\n",
            "         1.98825195e-01,  2.29354292e-01],\n",
            "       [-4.36674841e-02,  1.15827471e-03,  4.17670496e-02,\n",
            "        -4.25934196e-02, -1.77706368e-02,  4.27177437e-02,\n",
            "        -2.29436513e-02, -5.01442701e-04],\n",
            "       [ 2.24702939e-01, -1.85867682e-01, -1.98614642e-01,\n",
            "         2.09242925e-01, -1.43887118e-01,  2.54656911e-01,\n",
            "         2.01280594e-01,  1.85402438e-01],\n",
            "       [-3.08881402e-02,  1.00613460e-02, -1.50492042e-03,\n",
            "         2.69985907e-02,  1.25007145e-02, -3.05637009e-02,\n",
            "         3.87883820e-02, -6.08553737e-03],\n",
            "       [-2.27708463e-02, -2.74214745e-02,  4.50923294e-03,\n",
            "        -2.01112274e-02,  1.52905248e-02,  6.52456284e-03,\n",
            "         3.51740047e-03, -2.05195677e-02],\n",
            "       [ 4.63422872e-02,  3.63316052e-02,  2.09471025e-02,\n",
            "        -4.61581349e-02, -4.37285751e-03, -1.51944160e-03,\n",
            "         4.36655022e-02, -4.31135669e-02],\n",
            "       [ 2.08693415e-01, -1.51605994e-01, -1.97971970e-01,\n",
            "         2.09340677e-01,  2.26692900e-01, -2.48981014e-01,\n",
            "        -1.53712541e-01, -2.01947391e-01],\n",
            "       [ 2.97718532e-02, -2.50088815e-02,  3.08146589e-02,\n",
            "         4.43980210e-02, -1.57503858e-02, -2.77104862e-02,\n",
            "        -8.78084451e-04,  9.51943547e-03]], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  first word, we get the following vector. is :\n",
        "# [-2.31203958e-01,  1.86820880e-01,  1.68216407e-01,\n",
        "  #      -2.04175085e-01,  1.68387517e-01, -2.67770767e-01,\n",
        "   #     -1.87188357e-01, -2.19602570e-01]"
      ],
      "metadata": {
        "id": "NK939gpC_40y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# text_to_word_sequence"
      ],
      "metadata": {
        "id": "0LIHmS_VeBq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# tokenize the document\n",
        "result = text_to_word_sequence(text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK3X2IgxeDGv",
        "outputId": "0057beae-156b-4801-e036-8b3a9995b19f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the document\n",
        "text = 'The quick brown fox jumped over the lazy dog.'\n",
        "# estimate the size of the vocabulary\n",
        "words = set(text_to_word_sequence(text))\n",
        "vocab_size = len(words)\n",
        "print(vocab_size)\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaER0WgaeGDw",
        "outputId": "9d2f5dc2-664e-4f00-de2c-6d6d663d6128"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n",
            "{'jumped', 'the', 'fox', 'lazy', 'dog', 'brown', 'quick', 'over'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converts a text to a sequence of indexes in a fixed-size hashing space.\n",
        "# A list of integer word indices (unicity non-guaranteed).\n",
        "#0 is a reserved index that won't be assigned to any word. Two or more words may be assigned to the same index, due to possible collisions by the hashing function. The probability of a collision is in relation to the dimension of the hashing space and the number of distinct objects."
      ],
      "metadata": {
        "id": "b9nj1YBvwrIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "tf.keras.preprocessing.text.hashing_trick(\n",
        "    text,\n",
        "    n,\n",
        "    hash_function=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    analyzer=None\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "oOYjynn5e5FM",
        "outputId": "c8fc1a13-bc09-43d3-b9ed-23f7db58924d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-0bdc26846b05>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tf.keras.preprocessing.text.hashing_trick(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhash_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-hot encodes a text into a list of word indexes of size n\n",
        "# This function receives as input a string of text and returns a list of encoded integers each corresponding to a word (or token) in the given input string.\n",
        "\n"
      ],
      "metadata": {
        "id": "0S0paFphw5sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "tf.keras.preprocessing.text.one_hot(\n",
        "    input_text,\n",
        "    n,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True,\n",
        "    split=' ',\n",
        "    analyzer=None\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "RH5dXpcqw65w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenize"
      ],
      "metadata": {
        "id": "_-KLnAdU3EIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        " 'Good work',\n",
        " 'Great effort',\n",
        " 'nice work',\n",
        " 'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)"
      ],
      "metadata": {
        "id": "oJWppiln3GzC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   word_counts: A dictionary of words and their counts.\n",
        "2.   word_docs: A dictionary of words and how many documents each appeared in.\n",
        "1.   document_count:An integer count of the total number of documents that were used to fit the Tokenizer.\n",
        "2.   word_index: A dictionary of words and their uniquely assigned integers.\n",
        "\n"
      ],
      "metadata": {
        "id": "zcacICJB3LF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvHRYDws3g6v",
        "outputId": "6d492b7a-154a-4f5d-ae55-96c8022c6ee4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "5\n",
            "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The texts_to_matrix() function on the Tokenizer can be used to create one vector per document provided per input. The length of the vectors is the total size of the vocabulary.\n",
        "\n",
        "This function provides a suite of standard bag-of-words model text encoding schemes that can be provided via a mode argument to the function.\n",
        "\n",
        "The modes available include:\n",
        "\n",
        "‘binary‘: Whether or not each word is present in the document. This is the default.\n",
        "‘count‘: The count of each word in the document.\n",
        "‘tfidf‘: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
        "‘freq‘: The frequency of each word as a ratio of words within each document."
      ],
      "metadata": {
        "id": "YlLoZ56452Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "# define 5 documents\n",
        "docs = ['Well done!',\n",
        " 'Good work',\n",
        " 'Great effort',\n",
        " 'nice work',\n",
        " 'Excellent!']\n",
        "# create the tokenizer\n",
        "t = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(docs)\n",
        "# summarize what was learned\n",
        "print(t.word_counts)\n",
        "print(t.document_count)\n",
        "print(t.word_index)\n",
        "print(t.word_docs)\n",
        "# integer encode documents\n",
        "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
        "print(encoded_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4eNtFRlG387w",
        "outputId": "3eb37b48-e84d-4c55-e740-22cd164a298c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
            "5\n",
            "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
            "defaultdict(<class 'int'>, {'well': 1, 'done': 1, 'work': 2, 'good': 1, 'effort': 1, 'great': 1, 'nice': 1, 'excellent': 1})\n",
            "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running the example above fits the Tokenizer with 5 small documents. The details of the fit Tokenizer are printed. Then the 5 documents are encoded using a word count.\n",
        "\n",
        "Each document is encoded as a 9-element vector with one position for each word and the chosen encoding scheme value for each word position. In this case, a simple word count mode is used."
      ],
      "metadata": {
        "id": "T9nqZLPh6D2G"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIjCV8It5-vB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}